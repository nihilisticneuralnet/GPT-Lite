{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83b4a316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `C:\\Users\\Acer\\.julia\\registries\\General.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Acer\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Acer\\Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"CUDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c2c5acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not functional. Please check your installation.\n"
     ]
    }
   ],
   "source": [
    "using CUDA\n",
    "\n",
    "if CUDA.functional()\n",
    "    println(\"CUDA is functional.\")\n",
    "    device_count = CUDA.device_count()\n",
    "    println(\"Number of CUDA devices: $device_count\")\n",
    "else\n",
    "    println(\"CUDA is not functional. Please check your installation.\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73942fad",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching cpu()\n\u001b[0mClosest candidates are:\n\u001b[0m  cpu(\u001b[91m::Any\u001b[39m) at C:\\Users\\Acer\\.julia\\packages\\Flux\\n3cOc\\src\\functor.jl:210",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching cpu()\n\u001b[0mClosest candidates are:\n\u001b[0m  cpu(\u001b[91m::Any\u001b[39m) at C:\\Users\\Acer\\.julia\\packages\\Flux\\n3cOc\\src\\functor.jl:210",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[4]:11"
     ]
    }
   ],
   "source": [
    "using Flux\n",
    "using Flux: @epochs, onehotbatch, mse, throttle\n",
    "using CUDA\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64\n",
    "block_size = 256\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = cpu()  \n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51bf185a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Random.TaskLocalRNG()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Random\n",
    "Random.seed!(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21267a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111540-element Vector{Int64}:\n",
       " 13\n",
       "  1\n",
       "  1\n",
       " 20\n",
       " 31\n",
       " 18\n",
       " 26\n",
       " 22\n",
       " 28\n",
       " 11\n",
       "  1\n",
       " 20\n",
       " 54\n",
       "  â‹®\n",
       " 40\n",
       " 57\n",
       " 59\n",
       "  2\n",
       " 62\n",
       " 40\n",
       " 50\n",
       " 48\n",
       " 53\n",
       " 46\n",
       "  9\n",
       "  1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the input text file\n",
    "text = \"\"\n",
    "open(\"input.txt\", \"r\") do f\n",
    "    global text = read(f, String)\n",
    "end\n",
    "\n",
    "# Create a set of unique characters\n",
    "chars = sort(collect(Set(text)))\n",
    "vocab_size = length(chars)\n",
    "\n",
    "# Create mappings from characters to integers and vice versa\n",
    "stoi = Dict(ch => i for (i, ch) in enumerate(chars))\n",
    "itos = Dict(i => ch for (i, ch) in enumerate(chars))\n",
    "\n",
    "# Define encoding and decoding functions\n",
    "encode(s) = [stoi[c] for c in s]\n",
    "decode(l) = join([itos[i] for i in l])\n",
    "\n",
    "# Train and test splits\n",
    "data = Int64.(encode(text))\n",
    "n = floor(Int, 0.9 * length(data))  # first 90% will be train, rest val\n",
    "train_data = data[1:n]\n",
    "val_data = data[n+1:end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cec96554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_batch (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_batch(split)\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = if split == \"train\"\n",
    "        train_data\n",
    "    else\n",
    "        val_data\n",
    "    end\n",
    "    ix = rand(1:length(data) - block_size, batch_size)\n",
    "    x = [data[i:i+block_size-1] for i in ix]\n",
    "    y = [data[i+1:i+block_size] for i in ix]\n",
    "    x, y = CUDA.CuArray(x), CUDA.CuArray(y)\n",
    "    return x, y\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1fe096b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Recur(\n",
       "    LSTMCell(65 => 384),                \u001b[90m# 691_968 parameters\u001b[39m\n",
       "  ),\n",
       "  Recur(\n",
       "    LSTMCell(384 => 384),               \u001b[90m# 1_181_952 parameters\u001b[39m\n",
       "  ),\n",
       "  Dense(384 => 65),                     \u001b[90m# 25_025 parameters\u001b[39m\n",
       "  NNlib.logsoftmax,\n",
       ") \u001b[90m        # Total: 12 trainable arrays, \u001b[39m1_898_945 parameters,\n",
       "\u001b[90m          # plus 4 non-trainable, 1_536 parameters, summarysize \u001b[39m7.245 MiB."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Chain(\n",
    "    LSTM(vocab_size, n_embd),\n",
    "    LSTM(n_embd, n_embd),\n",
    "    Dense(n_embd, vocab_size),\n",
    "    logsoftmax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5adfbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "estimate_loss (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the `estimate_loss` function with the defined model\n",
    "function estimate_loss()\n",
    "    out = Dict{String, Float64}()\n",
    "    Flux.eval!(model)\n",
    "    for split in [\"train\", \"val\"]\n",
    "        losses = zeros(eval_iters)\n",
    "        for k in 1:eval_iters\n",
    "            X, Y = get_batch(split)\n",
    "            logits = model(X)\n",
    "            loss = mse(logits, Y)\n",
    "            losses[k] = Flux.item(loss)\n",
    "        end\n",
    "        out[split] = mean(losses)\n",
    "    end\n",
    "    Flux.train!(model)\n",
    "    return out\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6759850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Head\n",
    "    key::Dense\n",
    "    query::Dense\n",
    "    value::Dense\n",
    "    tril::CuArray{Float32}\n",
    "\n",
    "    function Head(head_size)\n",
    "        key = Dense(n_embd, head_size; bias=false)\n",
    "        query = Dense(n_embd, head_size; bias=false)\n",
    "        value = Dense(n_embd, head_size; bias=false)\n",
    "        tril = CUDA.ones(Float32, block_size, block_size)\n",
    "        tril = Flux.tril(tril)\n",
    "        dropout = Dropout(dropout)\n",
    "\n",
    "        new(key, query, value, tril)\n",
    "    end\n",
    "end\n",
    "\n",
    "function (head::Head)(x)\n",
    "    B, T, C = size(x)\n",
    "    k = head.key(x)   # (B, T, hs)\n",
    "    q = head.query(x) # (B, T, hs)\n",
    "    wei = q * permutedims(k, (1, 3, 2)) * (size(k, 2)^-0.5) # (B, T, hs) * (B, hs, T) -> (B, T, T)\n",
    "    wei[head.tril[T, T] .== 0] .= -Inf # (B, T, T)\n",
    "    wei = softmax(wei, dims=3) # (B, T, T)\n",
    "    wei = head.dropout(wei)\n",
    "    v = head.value(x) # (B, T, hs)\n",
    "    out = wei * v # (B, T, T) * (B, T, hs) -> (B, T, hs)\n",
    "    return out\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b543828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MultiHeadAttention\n",
    "    heads::Vector{Head}\n",
    "    proj::Dense\n",
    "    dropout::Dropout\n",
    "\n",
    "    function MultiHeadAttention(num_heads, head_size)\n",
    "        heads = [Head(head_size) for _ in 1:num_heads]\n",
    "        proj = Dense(head_size * num_heads, n_embd)\n",
    "        dropout = Dropout(dropout)\n",
    "\n",
    "        new(heads, proj, dropout)\n",
    "    end\n",
    "end\n",
    "\n",
    "function (attention::MultiHeadAttention)(x)\n",
    "    out = cat([h(x) for h in attention.heads]...; dims=3)\n",
    "    out = attention.dropout(attention.proj(out))\n",
    "    return out\n",
    "end\n",
    "\n",
    "struct FeedFoward\n",
    "    net::Chain\n",
    "\n",
    "    function FeedFoward(n_embd)\n",
    "        net = Chain(\n",
    "            Dense(n_embd, 4n_embd),\n",
    "            relu,\n",
    "            Dense(4n_embd, n_embd),\n",
    "            Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        new(net)\n",
    "    end\n",
    "end\n",
    "\n",
    "function (ffn::FeedFoward)(x)\n",
    "    return ffn.net(x)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dcf0b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Block\n",
    "    sa::MultiHeadAttention\n",
    "    ffwd::FeedFoward\n",
    "    ln1::LayerNorm\n",
    "    ln2::LayerNorm\n",
    "\n",
    "    function Block(n_embd, n_head)\n",
    "        head_size = div(n_embd, n_head)\n",
    "        sa = MultiHeadAttention(n_head, head_size)\n",
    "        ffwd = FeedFoward(n_embd)\n",
    "        ln1 = LayerNorm(n_embd)\n",
    "        ln2 = LayerNorm(n_embd)\n",
    "\n",
    "        new(sa, ffwd, ln1, ln2)\n",
    "    end\n",
    "end\n",
    "\n",
    "function (block::Block)(x)\n",
    "    x = x + block.sa(block.ln1(x))\n",
    "    x = x + block.ffwd(block.ln2(x))\n",
    "    return x\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60fa8b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct GPTLanguageModel\n",
    "    token_embedding_table::Embedding\n",
    "    position_embedding_table::Embedding\n",
    "    blocks::Chain\n",
    "    ln_f::LayerNorm\n",
    "    lm_head::Dense\n",
    "end\n",
    "\n",
    "function _init_weights(model)\n",
    "    for (name, param) in Flux.params(model)\n",
    "        if isa(param, AbstractConv)\n",
    "            param.weight.data .= Flux.glorot_normal(param.weight.data)\n",
    "            if param.bias !== nothing\n",
    "                param.bias.data .= 0\n",
    "            end\n",
    "        elseif isa(param, Embed)\n",
    "            param.weight.data .= Flux.glorot_normal(param.weight.data)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function GPTLanguageModel()\n",
    "    token_embedding_table = Embedding(vocab_size, n_embd)\n",
    "    position_embedding_table = Embedding(block_size, n_embd)\n",
    "    blocks = Chain([Block(n_embd, n_head) for _ in 1:n_layer]...)\n",
    "    ln_f = LayerNorm(n_embd)\n",
    "    lm_head = Dense(n_embd, vocab_size)\n",
    "\n",
    "    model = GPTLanguageModel(token_embedding_table, position_embedding_table, blocks, ln_f, lm_head)\n",
    "    _init_weights(model)\n",
    "\n",
    "    return model\n",
    "end\n",
    "\n",
    "function (model::GPTLanguageModel)(idx; targets = nothing)\n",
    "    B, T = size(idx)\n",
    "\n",
    "    # idx and targets are both (B,T) tensor of integers\n",
    "    tok_emb = model.token_embedding_table(idx) # (B,T,C)\n",
    "    pos_emb = model.position_embedding_table(collect(1:T)) # (T,C)\n",
    "    x = tok_emb + pos_emb # (B,T,C)\n",
    "    x = model.blocks(x) # (B,T,C)\n",
    "    x = model.ln_f(x) # (B,T,C)\n",
    "    logits = model.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "    if isnothing(targets)\n",
    "        loss = nothing\n",
    "    else\n",
    "        logits = Flux.reshape(logits, (B*T, vocab_size))\n",
    "        targets = Flux.reshape(targets, (B*T,))\n",
    "        loss = Flux.crossentropy(logits, targets)\n",
    "    end\n",
    "\n",
    "    return logits, loss\n",
    "end\n",
    "\n",
    "function generate(model::GPTLanguageModel, idx, max_new_tokens)\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in 1:max_new_tokens\n",
    "        # crop idx to the last block_size tokens\n",
    "        idx_cond = idx[:, end-block_size+1:end]\n",
    "        # get the predictions\n",
    "        logits, loss = model(idx_cond)\n",
    "        # focus only on the last time step\n",
    "        logits = logits[:, end, :] # becomes (B, C)\n",
    "        # apply softmax to get probabilities\n",
    "        probs = Flux.softmax(logits, dims=2) # (B, C)\n",
    "        # sample from the distribution\n",
    "        idx_next = Flux.multinomial(probs, 1) # (B, 1)\n",
    "        # append sampled index to the running sequence\n",
    "        idx = hcat(idx, idx_next) # (B, T+1)\n",
    "    end\n",
    "    return idx\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "300536b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "CUDA driver not found",
     "output_type": "error",
     "traceback": [
      "CUDA driver not found",
      "",
      "Stacktrace:",
      "  [1] error(s::String)",
      "    @ Base .\\error.jl:35",
      "  [2] functional",
      "    @ C:\\Users\\Acer\\.julia\\packages\\CUDA\\pCcGc\\src\\initialization.jl:24 [inlined]",
      "  [3] task_local_state!()",
      "    @ CUDA C:\\Users\\Acer\\.julia\\packages\\CUDA\\pCcGc\\lib\\cudadrv\\state.jl:77",
      "  [4] active_state",
      "    @ C:\\Users\\Acer\\.julia\\packages\\CUDA\\pCcGc\\lib\\cudadrv\\state.jl:112 [inlined]",
      "  [5] #_alloc#989",
      "    @ C:\\Users\\Acer\\.julia\\packages\\CUDA\\pCcGc\\src\\pool.jl:409 [inlined]",
      "  [6] #alloc#988",
      "    @ C:\\Users\\Acer\\.julia\\packages\\CUDA\\pCcGc\\src\\pool.jl:398 [inlined]",
      "  [7] alloc",
      "    @ C:\\Users\\Acer\\.julia\\packages\\CUDA\\pCcGc\\src\\pool.jl:392 [inlined]",
      "  [8] CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}(#unused#::UndefInitializer, dims::Tuple{Int64, Int64})",
      "    @ CUDA C:\\Users\\Acer\\.julia\\packages\\CUDA\\pCcGc\\src\\array.jl:93",
      "  [9] CuArray",
      "    @ C:\\Users\\Acer\\.julia\\packages\\CUDA\\pCcGc\\src\\array.jl:176 [inlined]",
      " [10] (CuArray{Float32})(#unused#::UndefInitializer, dims::Tuple{Int64, Int64})",
      "    @ CUDA C:\\Users\\Acer\\.julia\\packages\\CUDA\\pCcGc\\src\\array.jl:187",
      " [11] (CuArray{Float32})(::UndefInitializer, ::Int64, ::Vararg{Int64})",
      "    @ CUDA C:\\Users\\Acer\\.julia\\packages\\CUDA\\pCcGc\\src\\array.jl:189",
      " [12] ones(::Type, ::Int64, ::Vararg{Int64})",
      "    @ CUDA C:\\Users\\Acer\\.julia\\packages\\CUDA\\pCcGc\\src\\array.jl:656",
      " [13] Head(head_size::Int64)",
      "    @ Main .\\In[11]:11",
      " [14] (::var\"#25#26\"{Int64})(#unused#::Int64)",
      "    @ Main .\\none:0",
      " [15] iterate",
      "    @ .\\generator.jl:47 [inlined]",
      " [16] collect",
      "    @ .\\array.jl:787 [inlined]",
      " [17] MultiHeadAttention(num_heads::Int64, head_size::Int64)",
      "    @ Main .\\In[12]:7",
      " [18] Block(n_embd::Int64, n_head::Int64)",
      "    @ Main .\\In[13]:9",
      " [19] (::var\"#31#32\")(#unused#::Int64)",
      "    @ Main .\\none:0",
      " [20] iterate",
      "    @ .\\generator.jl:47 [inlined]",
      " [21] collect(itr::Base.Generator{UnitRange{Int64}, var\"#31#32\"})",
      "    @ Base .\\array.jl:787",
      " [22] GPTLanguageModel()",
      "    @ Main .\\In[15]:25",
      " [23] top-level scope",
      "    @ In[16]:1"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel()\n",
    "m = model |> device\n",
    "# print the number of parameters in the model\n",
    "params_count = sum(p -> length(Flux.params(p)), Flux.children(m)) / 1e6\n",
    "println(\"$(params_count) M parameters\")\n",
    "\n",
    "# create a Flux optimizer\n",
    "optimizer = ADAMW(Flux.params(m), learning_rate)\n",
    "\n",
    "for iter in 1:max_iters\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 || iter == max_iters\n",
    "        losses = estimate_loss()\n",
    "        println(\"step $iter: train loss $(losses[\"train\"]:.4f), val loss $(losses[\"val\"]:.4f)\")\n",
    "    end\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    Flux.reset!(optimizer)\n",
    "    Flux.back!(loss)\n",
    "    Flux.update!(optimizer, Flux.params(m))\n",
    "end\n",
    "\n",
    "# generate from the model\n",
    "context = CUDA.zeros((1, 1), dtype=Int64) |> device\n",
    "generated = generate(model, context, max_new_tokens=500)\n",
    "println(decode(generated[1, :]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00f14f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
